{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string('checkpoints_dir', 'checkpoints/1512582981-taskC(50,5,0.3).txt',\n",
    "                       'Checkpoints directory (example: checkpoints/1479670630). Must contain (at least):\\n'\n",
    "                       '- config.pkl: Contains parameters used to train the model \\n'\n",
    "                       '- model.ckpt: Contains the weights of the model \\n'\n",
    "                       '- model.ckpt.meta: Contains the TensorFlow graph definition \\n')\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring graph ...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/1512582981-taskC(50,5,0.3).txt/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/1512582981-taskC(50,5,0.3).txt/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "writer = open('test/results.txt', 'a+t')\n",
    "RESULT = \"\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Import graph and restore its weights\n",
    "    print('Restoring graph ...')\n",
    "    saver = tf.train.import_meta_graph(\"{}/model.ckpt.meta\".format(FLAGS.checkpoints_dir))\n",
    "    saver.restore(sess, (\"{}/model.ckpt\".format(FLAGS.checkpoints_dir)))\n",
    "\n",
    "    # Recover input/output tensors\n",
    "    input = graph.get_operation_by_name('input').outputs[0]\n",
    "    target = graph.get_operation_by_name('target').outputs[0]\n",
    "    seq_len = graph.get_operation_by_name('lengths').outputs[0]\n",
    "    dropout_keep_prob = graph.get_operation_by_name('dropout_keep_prob').outputs[0]\n",
    "    predict = graph.get_operation_by_name('final_layer/softmax/predictions').outputs[0]\n",
    "    accuracy = graph.get_operation_by_name('accuracy/accuracy').outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_val = 1000\n",
    "pos_val = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length : 7790\n",
      "data shape : (7790, 30, 1039)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "with open('data/taskB(1000,10,0.3).txt', 'r') as f:\n",
    "    file = f.readlines()\n",
    "    file = [e.split('\\n')[:-1][0] for e in file]\n",
    "    idxs = [i for i, x in enumerate(file) if x == '*']\n",
    "    tweets = []\n",
    "    for i in range(len(idxs) - 1):\n",
    "        tweet = file[idxs[i]+1:idxs[i+1]]\n",
    "        tweets.append(tweet)\n",
    "\n",
    "seqlens = list(map(int, [tweet[0] for tweet in tweets]))\n",
    "raw_labels = [tweet[1] for tweet in tweets]\n",
    "outliers_idx = [i for i,x in enumerate(seqlens) if x > 30]\n",
    "seqlens = [x for i, x in enumerate(seqlens) if i not in outliers_idx]\n",
    "raw_labels = [x for i, x in enumerate(raw_labels) if i not in outliers_idx]\n",
    "\n",
    "label_datas = []\n",
    "idx = 0\n",
    "for i, tweet in enumerate(tweets):\n",
    "    if i in outliers_idx:\n",
    "        continue\n",
    "    label_data = np.zeros((max(seqlens), k_val+pos_val+1))\n",
    "    for i, line in enumerate(tweet[2:]):\n",
    "        n1 = np.zeros(k_val+1, dtype=np.float)\n",
    "        for idx in line.split('/')[0].split(', '):\n",
    "            n1[int(idx)] = 1\n",
    "        n2 = np.zeros(pos_val, dtype=np.float)\n",
    "        idx2 = int(line.split('/')[1:][0])\n",
    "        n2[idx2] = 1\n",
    "        n2[-1] = line.split('/')[1:][1]\n",
    "        label_data[i] = np.append(n1, n2)\n",
    "    label_datas.append(label_data)\n",
    "label_datas = np.array(label_datas, dtype=np.float32)\n",
    "\n",
    "seqlens = [sum(label_datas[i].any(axis=1)) for i in range(len(label_datas))]\n",
    "assert sum(label_datas[i].any(axis=1)) == seqlens[i]\n",
    "seqlens = np.array(seqlens)\n",
    "\n",
    "print (\"data length : {}\".format(len(label_datas)))\n",
    "print (\"data shape : {}\".format(str(label_datas.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counter=collections.Counter(raw_labels)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labels = np.asarray(deepcopy(raw_labels))\n",
    "enc = LabelEncoder()\n",
    "labels = enc.fit_transform(labels).reshape(-1, 1)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "labels = ohe.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7790, 30, 1039)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = label_datas\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = np.dstack(b, np.zeros(16095, 2, 89))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.concatenate((a, np.zeros((a.shape[0], 32-a.shape[1], a.shape[2]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7790, 32, 1039)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
