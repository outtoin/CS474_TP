{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT = \"checkpoints/1512591657-taskA(1000,3,0.3).txt\"\n",
    "TESTDIR = \"test/taskA(1000,3,0.3).txt\"\n",
    "KVAL = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length : 16169\n",
      "data shape : (16169, 30, 1039)\n",
      "x_test: (16169, 30, 1039)\n",
      "y_test: (16169, 3)\n",
      "test_seq_len: (16169,)\n"
     ]
    }
   ],
   "source": [
    "RESULTDIR = TESTDIR.split('/')[0]\n",
    "if CHECKPOINT == \"\" or TESTDIR == \"\" or KVAL == 0:\n",
    "    sys.exit(1)\n",
    "\n",
    "tf.flags.DEFINE_string('checkpoints_dir', CHECKPOINT,\n",
    "                       'Checkpoints directory (example: checkpoints/1479670630). Must contain (at least):\\n'\n",
    "                       '- config.pkl: Contains parameters used to train the model \\n'\n",
    "                       '- model.ckpt: Contains the weights of the model \\n'\n",
    "                       '- model.ckpt.meta: Contains the TensorFlow graph definition \\n')\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "# Preprocessing Params\n",
    "pos_val = 38\n",
    "k_val = KVAL\n",
    "\n",
    "# Data Preprocessing\n",
    "with open(TESTDIR, 'r') as f:\n",
    "    file = f.readlines()\n",
    "    file = [e.split('\\n')[:-1][0] for e in file]\n",
    "    idxs = [i for i, x in enumerate(file) if x == '*']\n",
    "    tweets = []\n",
    "    for i in range(len(idxs) - 1):\n",
    "        tweet = file[idxs[i]+1:idxs[i+1]]\n",
    "        tweets.append(tweet)\n",
    "\n",
    "seqlens = list(map(int, [tweet[0] for tweet in tweets]))\n",
    "raw_labels = [tweet[1] for tweet in tweets]\n",
    "outliers_idx = [i for i,x in enumerate(seqlens) if x > 30]\n",
    "seqlens = [x for i, x in enumerate(seqlens) if i not in outliers_idx]\n",
    "raw_labels = [x for i, x in enumerate(raw_labels) if i not in outliers_idx]\n",
    "\n",
    "label_datas = []\n",
    "idx = 0\n",
    "for i, tweet in enumerate(tweets):\n",
    "    if i in outliers_idx:\n",
    "        continue\n",
    "    label_data = np.zeros((30, k_val+pos_val+1))\n",
    "    for i, line in enumerate(tweet[2:]):\n",
    "        n1 = np.zeros(k_val+1, dtype=np.float)\n",
    "        for idx in line.split('/')[0].split(', '):\n",
    "            n1[int(idx)] = 1\n",
    "        n2 = np.zeros(pos_val, dtype=np.float)\n",
    "        idx2 = int(line.split('/')[1:][0])\n",
    "        n2[idx2] = 1\n",
    "        n2[-1] = line.split('/')[1:][1]\n",
    "        label_data[i] = np.append(n1, n2)\n",
    "    label_datas.append(label_data)\n",
    "label_datas = np.array(label_datas, dtype=np.float32)\n",
    "\n",
    "seqlens = [sum(label_datas[i].any(axis=1)) for i in range(len(label_datas))]\n",
    "assert sum(label_datas[i].any(axis=1)) == seqlens[i]\n",
    "seqlens = np.array(seqlens)\n",
    "\n",
    "print (\"data length : {}\".format(len(label_datas)))\n",
    "print (\"data shape : {}\".format(str(label_datas.shape)))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labels = np.asarray(deepcopy(raw_labels))\n",
    "enc = LabelEncoder()\n",
    "labels = enc.fit_transform(labels).reshape(-1, 1)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "labels = ohe.fit_transform(labels)\n",
    "\n",
    "x_test = label_datas\n",
    "y_test = labels\n",
    "test_seq_len = seqlens\n",
    "\n",
    "print(\"x_test: {}\".format(x_test.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))\n",
    "print(\"test_seq_len: {}\".format(test_seq_len.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring graph ...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/1512591657-taskA(1000,3,0.3).txt/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/1512591657-taskA(1000,3,0.3).txt/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'except Exception as e:\\n    print(e)\\n    RESULT = \"TRAIN MODEL DOESN\\'T EXIST(TRAIN FAILED)\"\\nfinally:\\n    writer.writelines(\"[{0}]: {1}\\n\".format(TESTDIR, RESULT))\\n    writer.close()\\n    sys.exit(1)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = open('{}/results.txt'.format(RESULTDIR), 'a+t')\n",
    "RESULT = \"\"\n",
    "#try:\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Import graph and restore its weights\n",
    "    print('Restoring graph ...')\n",
    "    saver = tf.train.import_meta_graph(\"{}/model.ckpt.meta\".format(FLAGS.checkpoints_dir))\n",
    "    saver.restore(sess, (\"{}/model.ckpt\".format(FLAGS.checkpoints_dir)))\n",
    "\n",
    "    # Recover input/output tensors\n",
    "    input = graph.get_operation_by_name('input').outputs[0]\n",
    "    target = graph.get_operation_by_name('target').outputs[0]\n",
    "    seq_len = graph.get_operation_by_name('lengths').outputs[0]\n",
    "    dropout_keep_prob = graph.get_operation_by_name('dropout_keep_prob').outputs[0]\n",
    "    predict = graph.get_operation_by_name('final_layer/softmax/predictions').outputs[0]\n",
    "    accuracy = graph.get_operation_by_name('accuracy/accuracy').outputs[0]\n",
    "\n",
    "    if input.shape[1] > x_test.shape[1]:\n",
    "        x_test = np.concatenate((x_test, np.zeros((x_test.shape[0], 30-x_test.shape[1]))))\n",
    "    elif input.shape[1] < x_test.shape[1]:\n",
    "        x_test = x_test[:, :input.shape[1], :]\n",
    "\n",
    "    # Perform prediction\n",
    "    pred, acc = sess.run([predict, accuracy],\n",
    "                         feed_dict={input: x_test,\n",
    "                                    target: y_test,\n",
    "                                    seq_len: test_seq_len,\n",
    "                                    dropout_keep_prob: 1})\n",
    "\n",
    "    # Print results\n",
    "    #print('\\nAccuracy: {0:.4f}\\n'.format(acc))\n",
    "    #RESULT = 'Accuracy: {0:.4f}'.format(acc)\n",
    "'''except Exception as e:\n",
    "    print(e)\n",
    "    RESULT = \"TRAIN MODEL DOESN'T EXIST(TRAIN FAILED)\"\n",
    "finally:\n",
    "    writer.writelines(\"[{0}]: {1}\\n\".format(TESTDIR, RESULT))\n",
    "    writer.close()\n",
    "    sys.exit(1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taskA/taskA(1000,3,0.3).dat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILENAME = TESTDIR.split('/')[1][:-4]\n",
    "LOGDIR = TESTDIR.split('/')[1][:5]\n",
    "LOGDIR\n",
    "'{0}/{1}.dat'.format(LOGDIR, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'taskA/taskA(1000,3,0.3).dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-48bebdf0d4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0}/{1}.dat\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOGDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'taskA/taskA(1000,3,0.3).dat'"
     ]
    }
   ],
   "source": [
    "pred.dump(\"{0}/{1}.dat\".format(LOGDIR, FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import fsync\n",
    "def sync(fh):\n",
    "\t\"\"\"\n",
    "\tThis makes sure data is written to disk, so that buffering doesn't influence the timings.\n",
    "\t\"\"\"\n",
    "\tfh.flush()\n",
    "\tfsync(fh.fileno())\n",
    "\n",
    "from numpy import save as np_save, load as np_load\n",
    "class NPY():\n",
    "\textension = 'npy'\n",
    "\tdef save(self, arr, pth):\n",
    "\t\twith open(pth, 'wb+') as fh:\n",
    "\t\t\tnp_save(fh, arr, allow_pickle=False)\n",
    "\t\t\tsync(fh)\n",
    "\t\t\n",
    "\tdef load(self, pth):\n",
    "\t\treturn np_load(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = NPY()\n",
    "writer.save(pred, \"a.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = writer.load(\"a.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
