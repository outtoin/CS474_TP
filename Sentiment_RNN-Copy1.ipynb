{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "from scipy import spatial\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "# miscellaneous\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/taskA(50,3,0.3).txt', 'r') as f:\n",
    "    file = f.readlines()\n",
    "    file = [e.split('\\n')[:-1][0] for e in file]\n",
    "    idxs = [i for i, x in enumerate(file) if x == '*']\n",
    "    tweets = []\n",
    "    for i in range(len(idxs) - 1):\n",
    "        tweet = file[idxs[i]+1:idxs[i+1]]\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqlens = list(map(int, [tweet[0] for tweet in tweets]))\n",
    "raw_labels = [tweet[1] for tweet in tweets]\n",
    "outliers_idx = [i for i,x in enumerate(seqlens) if x > 30]\n",
    "seqlens = [x for i, x in enumerate(seqlens) if i not in outliers_idx]\n",
    "raw_labels = [x for i, x in enumerate(raw_labels) if i not in outliers_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_datas = []\n",
    "idx = 0\n",
    "for i, tweet in enumerate(tweets):\n",
    "    if i in outliers_idx:\n",
    "        continue\n",
    "    label_data = np.zeros((max(seqlens), 5))\n",
    "    for i, line in enumerate(tweet[2:]):\n",
    "        n1 = np.array(line.split('/')[0].split(', '), dtype=np.int32)\n",
    "        n2 = np.array(line.split('/')[1:], dtype=np.int32)\n",
    "        label_data[i] = np.append(n1, n2)\n",
    "    label_datas.append(label_data)\n",
    "label_datas = np.array(label_datas, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqlens = [sum(label_datas[i].any(axis=1)) for i in range(len(label_datas))]\n",
    "assert sum(label_datas[i].any(axis=1)) == seqlens[i]\n",
    "seqlens = np.array(seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length : 11573\n"
     ]
    }
   ],
   "source": [
    "print (\"data length : {}\".format(len(label_datas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labels = np.asarray(deepcopy(raw_labels))\n",
    "enc = LabelEncoder()\n",
    "labels = enc.fit_transform(labels).reshape(-1, 1)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "labels = ohe.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_len = len(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(9258, 30, 5) \n",
      "Validation set: \t(1157, 30, 5) \n",
      "Test set: \t\t(1158, 30, 5)\n",
      "Train label set: \t(9258, 3) \n",
      "Validation label set: \t(1157, 3) \n",
      "Test label set: \t(1158, 3)\n",
      "Train seq set: \t\t(9258,) \n",
      "Validation seq set: \t(1157,) \n",
      "Test seq set: \t\t(1158,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_index = int(split_frac * len(label_datas))\n",
    "\n",
    "train_x, val_x = label_datas[:split_index], label_datas[split_index:]\n",
    "train_y, val_y = labels[:split_index], labels[split_index:]\n",
    "train_seq, val_seq = seqlens[:split_index], seqlens[split_index:]\n",
    "\n",
    "split_frac = 0.5\n",
    "split_index = int(split_frac * len(val_x))\n",
    "\n",
    "val_x, test_x = val_x[:split_index], val_x[split_index:]\n",
    "val_y, test_y = val_y[:split_index], val_y[split_index:]\n",
    "val_seq, test_seq = val_seq[:split_index], val_seq[split_index:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"Train label set: \\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation label set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest label set: \\t{}\".format(test_y.shape))\n",
    "print(\"Train seq set: \\t\\t{}\".format(train_seq.shape), \n",
    "      \"\\nValidation seq set: \\t{}\".format(val_seq.shape),\n",
    "      \"\\nTest seq set: \\t\\t{}\".format(test_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer('n_samples', None,\n",
    "                        'Number of samples to use from the dataset. Set n_samples=None to use the whole dataset')\n",
    "tf.flags.DEFINE_string('checkpoints_root', 'checkpoints',\n",
    "                       'Checkpoints directory. Parameters will be saved there')\n",
    "tf.flags.DEFINE_string('summaries_dir', 'logs',\n",
    "                       'Directory where TensorFlow summaries will be stored')\n",
    "tf.flags.DEFINE_integer('batch_size', 100,\n",
    "                        'Batch size')\n",
    "tf.flags.DEFINE_integer('train_steps', 500,\n",
    "                        'Number of training steps')\n",
    "tf.flags.DEFINE_integer('hidden_size', 75,\n",
    "                        'Hidden size of LSTM layer')\n",
    "tf.flags.DEFINE_integer('random_state', 0,\n",
    "                        'Random state used for data splitting. Default is 0')\n",
    "tf.flags.DEFINE_float('learning_rate', 0.01,\n",
    "                      'RMSProp learning rate')\n",
    "tf.flags.DEFINE_float('dropout_keep_prob', 0.5,\n",
    "                      '0<dropout_keep_prob<=1. Dropout keep-probability')\n",
    "tf.flags.DEFINE_integer('sequence_len', None,\n",
    "                        'Maximum sequence length. Let m be the maximum sequence length in the'\n",
    "                        ' dataset. Then, it\\'s required that sequence_len >= m. If sequence_len'\n",
    "                        ' is None, then it\\'ll be automatically assigned to m')\n",
    "tf.flags.DEFINE_integer('validate_every', 100,\n",
    "                        'Step frequency in order to evaluate the model using a validation set')\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, hidden_size, max_length, n_classes=3, learning_rate=0.01,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Builds a TensorFlow LSTM model\n",
    "        :param hidden_size: Array holding the number of units in the LSTM cell of each rnn layer\n",
    "        :param vocab_size: Vocabulary size (number of possible words that may appear in a sample)\n",
    "        :param embedding_size: Words will be encoded using a vector of this size\n",
    "        :param max_length: Maximum length of an input tensor\n",
    "        :param n_classes: Number of classification classes\n",
    "        :param learning_rate: Learning rate of RMSProp algorithm\n",
    "        :param random_state: Random state for dropout\n",
    "        \"\"\"\n",
    "        # Build TensorFlow graph\n",
    "        self.input = self.__input(max_length)\n",
    "        self.seq_len = self.__seq_len()\n",
    "        self.target = self.__target(n_classes)\n",
    "        self.dropout_keep_prob = self.__dropout_keep_prob()\n",
    "        #self.word_embeddings = self.__word_embeddings(self.input, vocab_size, embedding_size, random_state)\n",
    "        self.scores = self.__scores(self.input, self.seq_len, hidden_size, n_classes, self.dropout_keep_prob,\n",
    "                                    random_state)\n",
    "        self.predict = self.__predict(self.scores)\n",
    "        self.losses = self.__losses(self.scores, self.target)\n",
    "        self.loss = self.__loss(self.losses)\n",
    "        self.train_step = self.__train_step(learning_rate, self.loss)\n",
    "        self.accuracy = self.__accuracy(self.predict, self.target)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "    def __input(self, max_length):\n",
    "        \"\"\"\n",
    "        :param max_length: Maximum length of an input tensor\n",
    "        :return: Input placeholder with shape [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        return tf.placeholder(tf.float32, [None, max_length, 5], name='input')\n",
    "\n",
    "    def __seq_len(self):\n",
    "        \"\"\"\n",
    "        :return: Sequence length placeholder with shape [batch_size]. Holds each tensor's real length in a given batch,\n",
    "                 allowing a dynamic sequence length.\n",
    "        \"\"\"\n",
    "        return tf.placeholder(tf.int32, [None], name='lengths')\n",
    "\n",
    "    def __target(self, n_classes):\n",
    "        \"\"\"\n",
    "        :param n_classes: Number of classification classes\n",
    "        :return: Target placeholder with shape [batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        return tf.placeholder(tf.float32, [None, n_classes], name='target')\n",
    "\n",
    "    def __dropout_keep_prob(self):\n",
    "        \"\"\"\n",
    "        :return: Placeholder holding the dropout keep probability\n",
    "        \"\"\"\n",
    "        return tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "    def __cell(self, hidden_size, dropout_keep_prob, seed=None):\n",
    "        \"\"\"\n",
    "        Builds a LSTM cell with a dropout wrapper\n",
    "        :param hidden_size: Number of units in the LSTM cell\n",
    "        :param dropout_keep_prob: Tensor holding the dropout keep probability\n",
    "        :param seed: Optional. Random state for the dropout wrapper\n",
    "        :return: LSTM cell with a dropout wrapper\n",
    "        \"\"\"\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "        dropout_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=dropout_keep_prob,\n",
    "                                                     output_keep_prob=dropout_keep_prob, seed=seed)\n",
    "        return dropout_cell\n",
    "\n",
    "    def __word_embeddings(self, x, vocab_size, embedding_size, seed=None):\n",
    "        \"\"\"\n",
    "        Builds the embedding layer with shape [vocab_size, embedding_size]\n",
    "        :param x: Input with shape [batch_size, max_length]\n",
    "        :param vocab_size: Vocabulary size (number of possible words that may appear in a sample)\n",
    "        :param embedding_size: Words will be represented using a vector of this size\n",
    "        :param seed: Optional. Random state for the embeddings initiallization\n",
    "        :return: Embedding lookup tensor with shape [batch_size, max_length, embedding_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('word_embeddings'):\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1, seed=seed))\n",
    "            embedded_words = tf.nn.embedding_lookup(embeddings, x)\n",
    "        return embedded_words\n",
    "\n",
    "    def __rnn_layer(self, hidden_size, x, seq_len, dropout_keep_prob, variable_scope=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Builds a LSTM layer\n",
    "        :param hidden_size: Number of units in the LSTM cell\n",
    "        :param x: Input with shape [batch_size, max_length]\n",
    "        :param seq_len: Sequence length tensor with shape [batch_size]\n",
    "        :param dropout_keep_prob: Tensor holding the dropout keep probability\n",
    "        :param variable_scope: Optional. Name of variable scope. Default is 'rnn_layer'\n",
    "        :param random_state: Optional. Random state for the dropout wrapper\n",
    "        :return: outputs with shape [batch_size, max_seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(variable_scope, default_name='rnn_layer'):\n",
    "            # Build LSTM cell\n",
    "            lstm_cell = self.__cell(hidden_size, dropout_keep_prob, random_state)\n",
    "\n",
    "            # Dynamically unroll LSTM cells according to seq_len. From TensorFlow documentation:\n",
    "            # \"The parameter `sequence_length` is used to copy-through state and zero-out outputs when past a batch\n",
    "            # element's sequence length.\"\n",
    "            outputs, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seq_len)\n",
    "        return outputs\n",
    "\n",
    "    def __scores(self, x, seq_len, hidden_size, n_classes, dropout_keep_prob, random_state=None):\n",
    "        \"\"\"\n",
    "        Builds the LSTM layers and the final fully connected layer\n",
    "        :param embedded_words: Embedding lookup tensor with shape [batch_size, max_length, embedding_size]\n",
    "        :param seq_len: Sequence length tensor with shape [batch_size]\n",
    "        :param hidden_size: Array holding the number of units in the LSTM cell of each rnn layer\n",
    "        :param n_classes: Number of classification classes\n",
    "        :param dropout_keep_prob: Tensor holding the dropout keep probability\n",
    "        :param random_state: Optional. Random state for the dropout wrapper\n",
    "        :return: Linear activation of each class with shape [batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        # Build LSTM layers\n",
    "        outputs = x\n",
    "        for h in hidden_size:\n",
    "            outputs = self.__rnn_layer(h, outputs, seq_len, dropout_keep_prob)\n",
    "\n",
    "        # Current shape of outputs: [batch_size, max_seq_len, hidden_size]. Reduce mean on index 1\n",
    "        outputs = tf.reduce_mean(outputs, reduction_indices=[1])\n",
    "\n",
    "        # Current shape of outputs: [batch_size, hidden_size]. Build fully connected layer\n",
    "        with tf.name_scope('final_layer/weights'):\n",
    "            w = tf.Variable(tf.truncated_normal([hidden_size[-1], n_classes], seed=random_state))\n",
    "            self.variable_summaries(w, 'final_layer/weights')\n",
    "        with tf.name_scope('final_layer/biases'):\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
    "            self.variable_summaries(b, 'final_layer/biases')\n",
    "        with tf.name_scope('final_layer/wx_plus_b'):\n",
    "            scores = tf.nn.xw_plus_b(outputs, w, b, name='scores')\n",
    "            tf.summary.histogram('final_layer/wx_plus_b', scores)\n",
    "        return scores\n",
    "\n",
    "    def __predict(self, scores):\n",
    "        \"\"\"\n",
    "        :param scores: Linear activation of each class with shape [batch_size, n_classes]\n",
    "        :return: Softmax activations with shape [batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('final_layer/softmax'):\n",
    "            softmax = tf.nn.softmax(scores, name='predictions')\n",
    "            tf.summary.histogram('final_layer/softmax', softmax)\n",
    "        return softmax\n",
    "\n",
    "    def __losses(self, scores, target):\n",
    "        \"\"\"\n",
    "        :param scores: Linear activation of each class with shape [batch_size, n_classes]\n",
    "        :param target: Target tensor with shape [batch_size, n_classes]\n",
    "        :return: Cross entropy losses with shape [batch_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=target)\n",
    "        return cross_entropy\n",
    "\n",
    "    def __loss(self, losses):\n",
    "        \"\"\"\n",
    "        :param losses: Cross entropy losses with shape [batch_size]\n",
    "        :return: Cross entropy loss mean\n",
    "        \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(losses, name='loss')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def __train_step(self, learning_rate, loss):\n",
    "        \"\"\"\n",
    "        :param learning_rate: Learning rate of RMSProp algorithm\n",
    "        :param loss: Cross entropy loss mean\n",
    "        :return: RMSProp train step operation\n",
    "        \"\"\"\n",
    "        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    def __accuracy(self, predict, target):\n",
    "        \"\"\"\n",
    "        :param predict: Softmax activations with shape [batch_size, n_classes]\n",
    "        :param target: Target tensor with shape [batch_size, n_classes]\n",
    "        :return: Accuracy mean obtained in current batch\n",
    "        \"\"\"\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_pred = tf.equal(tf.argmax(predict, 1), tf.argmax(target, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def initialize_all_variables(self):\n",
    "        \"\"\"\n",
    "        :return: Operation that initiallizes all variables\n",
    "        \"\"\"\n",
    "        return tf.global_variables_initializer()\n",
    "\n",
    "    @staticmethod\n",
    "    def variable_summaries(var, name):\n",
    "        \"\"\"\n",
    "        Attach a lot of summaries to a Tensor for Tensorboard visualization.\n",
    "        Ref: https://www.tensorflow.org/versions/r0.11/how_tos/summaries_and_tensorboard/index.html\n",
    "        :param var: Variable to summarize\n",
    "        :param name: Summary name\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean/' + name, mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev/' + name, stddev)\n",
    "            tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "            tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "            tf.summary.histogram(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare summaries\n",
    "summaries_dir = '{0}/{1}'.format(FLAGS.summaries_dir,\n",
    "                                 datetime.datetime.now().strftime('%d_%b_%Y-%H_%M_%S'))\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train')\n",
    "validation_writer = tf.summary.FileWriter(summaries_dir + '/validation')\n",
    "\n",
    "# Prepare model directory\n",
    "model_name = str(int(time.time()))\n",
    "model_dir = '{0}/{1}'.format(FLAGS.checkpoints_root, model_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# Save configuration\n",
    "FLAGS._parse_flags()\n",
    "config = FLAGS.__dict__['__flags']\n",
    "with open('{}/config.pkl'.format(model_dir), 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(hidden_size=[FLAGS.hidden_size],\n",
    "                   max_length=30,\n",
    "                   learning_rate=FLAGS.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, seq, batch_size):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y, seq = x[:n_batches*batch_size], y[:n_batches*batch_size], seq[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size], seq[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/500 train loss: 2.1796\n",
      "2/500 train loss: 1.4375\n",
      "3/500 train loss: 1.2472\n",
      "4/500 train loss: 1.0542\n",
      "5/500 train loss: 1.0639\n",
      "6/500 train loss: 1.0790\n",
      "7/500 train loss: 1.0479\n",
      "8/500 train loss: 1.0015\n",
      "9/500 train loss: 1.0573\n",
      "10/500 train loss: 1.0746\n",
      "11/500 train loss: 1.0778\n",
      "12/500 train loss: 1.0719\n",
      "13/500 train loss: 0.9759\n",
      "14/500 train loss: 1.0307\n",
      "15/500 train loss: 1.0053\n",
      "16/500 train loss: 1.0192\n",
      "17/500 train loss: 1.0657\n",
      "18/500 train loss: 1.0335\n",
      "19/500 train loss: 0.9996\n",
      "20/500 train loss: 1.0276\n",
      "21/500 train loss: 1.1233\n",
      "22/500 train loss: 0.9635\n",
      "23/500 train loss: 0.9973\n",
      "24/500 train loss: 0.9942\n",
      "25/500 train loss: 1.0376\n",
      "26/500 train loss: 1.0422\n",
      "27/500 train loss: 1.0279\n",
      "28/500 train loss: 1.0644\n",
      "29/500 train loss: 1.0148\n",
      "30/500 train loss: 1.0416\n",
      "31/500 train loss: 1.0529\n",
      "32/500 train loss: 1.0404\n",
      "33/500 train loss: 1.0417\n",
      "34/500 train loss: 1.0413\n",
      "35/500 train loss: 1.0623\n",
      "36/500 train loss: 1.0510\n",
      "37/500 train loss: 1.0594\n",
      "38/500 train loss: 1.0534\n",
      "39/500 train loss: 1.0249\n",
      "40/500 train loss: 0.9837\n",
      "41/500 train loss: 1.0569\n",
      "42/500 train loss: 1.1069\n",
      "43/500 train loss: 0.9954\n",
      "44/500 train loss: 1.1217\n",
      "45/500 train loss: 1.0462\n",
      "46/500 train loss: 1.0625\n",
      "47/500 train loss: 0.9908\n",
      "48/500 train loss: 1.0241\n",
      "49/500 train loss: 1.0804\n",
      "50/500 train loss: 1.0372\n",
      "51/500 train loss: 1.0562\n",
      "52/500 train loss: 1.0382\n",
      "53/500 train loss: 1.0550\n",
      "54/500 train loss: 1.0457\n",
      "55/500 train loss: 1.1367\n",
      "56/500 train loss: 1.0377\n",
      "57/500 train loss: 1.0084\n",
      "58/500 train loss: 1.0108\n",
      "59/500 train loss: 1.0520\n",
      "60/500 train loss: 1.0324\n",
      "61/500 train loss: 1.0442\n",
      "62/500 train loss: 1.0169\n",
      "63/500 train loss: 1.0557\n",
      "64/500 train loss: 1.0791\n",
      "65/500 train loss: 1.0638\n",
      "66/500 train loss: 1.0422\n",
      "67/500 train loss: 0.9748\n",
      "68/500 train loss: 1.0286\n",
      "69/500 train loss: 1.0403\n",
      "70/500 train loss: 1.0109\n",
      "71/500 train loss: 1.0105\n",
      "72/500 train loss: 0.9953\n",
      "73/500 train loss: 1.0555\n",
      "74/500 train loss: 1.0683\n",
      "75/500 train loss: 1.0506\n",
      "76/500 train loss: 1.0121\n",
      "77/500 train loss: 1.1077\n",
      "78/500 train loss: 1.0635\n",
      "79/500 train loss: 0.9892\n",
      "80/500 train loss: 1.0622\n",
      "81/500 train loss: 1.0342\n",
      "82/500 train loss: 1.0201\n",
      "83/500 train loss: 1.0416\n",
      "84/500 train loss: 1.0093\n",
      "85/500 train loss: 1.0401\n",
      "86/500 train loss: 0.9674\n",
      "87/500 train loss: 0.9796\n",
      "88/500 train loss: 1.0344\n",
      "89/500 train loss: 0.9970\n",
      "90/500 train loss: 1.0211\n",
      "91/500 train loss: 0.9989\n",
      "92/500 train loss: 1.0389\n",
      "93/500 train loss: 1.0649\n",
      "94/500 train loss: 1.0087\n",
      "95/500 train loss: 0.9805\n",
      "96/500 train loss: 1.0127\n",
      "97/500 train loss: 0.9914\n",
      "98/500 train loss: 1.0292\n",
      "99/500 train loss: 1.0322\n",
      "100/500 train loss: 0.9909\n",
      "[validation loss] 1.0773 (accuracy 0.4624)\n",
      "101/500 train loss: 1.0089\n",
      "102/500 train loss: 0.9742\n",
      "103/500 train loss: 1.0061\n",
      "104/500 train loss: 1.0220\n",
      "105/500 train loss: 1.0319\n",
      "106/500 train loss: 0.9817\n",
      "107/500 train loss: 1.0186\n",
      "108/500 train loss: 1.0278\n",
      "109/500 train loss: 1.1119\n",
      "110/500 train loss: 1.0267\n",
      "111/500 train loss: 0.9905\n",
      "112/500 train loss: 0.9878\n",
      "113/500 train loss: 0.9803\n",
      "114/500 train loss: 1.0232\n",
      "115/500 train loss: 0.9568\n",
      "116/500 train loss: 0.9425\n",
      "117/500 train loss: 0.9742\n",
      "118/500 train loss: 0.9581\n",
      "119/500 train loss: 0.9746\n",
      "120/500 train loss: 0.9692\n",
      "121/500 train loss: 0.9988\n",
      "122/500 train loss: 0.9892\n",
      "123/500 train loss: 0.9367\n",
      "124/500 train loss: 0.9648\n",
      "125/500 train loss: 1.0257\n",
      "126/500 train loss: 0.9707\n",
      "127/500 train loss: 0.9598\n",
      "128/500 train loss: 0.9633\n",
      "129/500 train loss: 0.9747\n",
      "130/500 train loss: 1.0256\n",
      "131/500 train loss: 1.0080\n",
      "132/500 train loss: 0.9783\n",
      "133/500 train loss: 0.9796\n",
      "134/500 train loss: 0.9761\n",
      "135/500 train loss: 1.0024\n",
      "136/500 train loss: 0.9948\n",
      "137/500 train loss: 0.9570\n",
      "138/500 train loss: 0.9765\n",
      "139/500 train loss: 0.9457\n",
      "140/500 train loss: 0.9849\n",
      "141/500 train loss: 1.0483\n",
      "142/500 train loss: 1.0412\n",
      "143/500 train loss: 0.9683\n",
      "144/500 train loss: 0.9732\n",
      "145/500 train loss: 0.9824\n",
      "146/500 train loss: 0.9638\n",
      "147/500 train loss: 1.0143\n",
      "148/500 train loss: 1.0133\n",
      "149/500 train loss: 0.9602\n",
      "150/500 train loss: 0.9576\n",
      "151/500 train loss: 0.9567\n",
      "152/500 train loss: 0.9634\n",
      "153/500 train loss: 1.0121\n",
      "154/500 train loss: 0.9721\n",
      "155/500 train loss: 1.0505\n",
      "156/500 train loss: 0.9979\n",
      "157/500 train loss: 1.0053\n",
      "158/500 train loss: 0.9911\n",
      "159/500 train loss: 0.9691\n",
      "160/500 train loss: 0.9818\n",
      "161/500 train loss: 0.9793\n",
      "162/500 train loss: 1.0061\n",
      "163/500 train loss: 0.9835\n",
      "164/500 train loss: 0.9961\n",
      "165/500 train loss: 0.9888\n",
      "166/500 train loss: 0.9710\n",
      "167/500 train loss: 1.0004\n",
      "168/500 train loss: 0.9227\n",
      "169/500 train loss: 0.9270\n",
      "170/500 train loss: 0.9765\n",
      "171/500 train loss: 0.9772\n",
      "172/500 train loss: 0.9715\n",
      "173/500 train loss: 0.9512\n",
      "174/500 train loss: 0.9574\n",
      "175/500 train loss: 0.9450\n",
      "176/500 train loss: 0.9729\n",
      "177/500 train loss: 0.9688\n",
      "178/500 train loss: 0.9861\n",
      "179/500 train loss: 0.8830\n",
      "180/500 train loss: 0.9749\n",
      "181/500 train loss: 0.9844\n",
      "182/500 train loss: 0.9215\n",
      "183/500 train loss: 0.9867\n",
      "184/500 train loss: 1.0210\n",
      "185/500 train loss: 0.9933\n",
      "186/500 train loss: 1.0275\n",
      "187/500 train loss: 0.9660\n",
      "188/500 train loss: 0.9484\n",
      "189/500 train loss: 0.9857\n",
      "190/500 train loss: 0.9553\n",
      "191/500 train loss: 0.9554\n",
      "192/500 train loss: 0.9546\n",
      "193/500 train loss: 0.9739\n",
      "194/500 train loss: 0.9736\n",
      "195/500 train loss: 0.9950\n",
      "196/500 train loss: 0.9490\n",
      "197/500 train loss: 0.9466\n",
      "198/500 train loss: 0.9947\n",
      "199/500 train loss: 0.9931\n",
      "200/500 train loss: 0.9673\n",
      "[validation loss] 1.0309 (accuracy 0.4659)\n",
      "201/500 train loss: 0.9287\n",
      "202/500 train loss: 0.9668\n",
      "203/500 train loss: 0.9021\n",
      "204/500 train loss: 0.9485\n",
      "205/500 train loss: 0.9555\n",
      "206/500 train loss: 0.9385\n",
      "207/500 train loss: 0.9759\n",
      "208/500 train loss: 0.9891\n",
      "209/500 train loss: 0.9719\n",
      "210/500 train loss: 0.9310\n",
      "211/500 train loss: 0.9753\n",
      "212/500 train loss: 0.9365\n",
      "213/500 train loss: 0.9354\n",
      "214/500 train loss: 0.9429\n",
      "215/500 train loss: 0.9379\n",
      "216/500 train loss: 0.9292\n",
      "217/500 train loss: 0.9524\n",
      "218/500 train loss: 0.9893\n",
      "219/500 train loss: 0.9351\n",
      "220/500 train loss: 1.0113\n",
      "221/500 train loss: 1.0203\n",
      "222/500 train loss: 0.9508\n",
      "223/500 train loss: 0.9647\n",
      "224/500 train loss: 0.9533\n",
      "225/500 train loss: 0.9748\n",
      "226/500 train loss: 0.9414\n",
      "227/500 train loss: 0.9608\n",
      "228/500 train loss: 0.9449\n",
      "229/500 train loss: 0.9487\n",
      "230/500 train loss: 0.9683\n",
      "231/500 train loss: 0.9036\n",
      "232/500 train loss: 0.9907\n",
      "233/500 train loss: 0.9671\n",
      "234/500 train loss: 1.0083\n",
      "235/500 train loss: 0.9627\n",
      "236/500 train loss: 0.9851\n",
      "237/500 train loss: 0.9551\n",
      "238/500 train loss: 0.9549\n",
      "239/500 train loss: 0.9534\n",
      "240/500 train loss: 0.9818\n",
      "241/500 train loss: 0.9083\n",
      "242/500 train loss: 0.9374\n",
      "243/500 train loss: 0.9579\n",
      "244/500 train loss: 0.9289\n",
      "245/500 train loss: 0.9792\n",
      "246/500 train loss: 0.9499\n",
      "247/500 train loss: 0.9617\n",
      "248/500 train loss: 0.9397\n",
      "249/500 train loss: 1.0093\n",
      "250/500 train loss: 0.9904\n",
      "251/500 train loss: 0.9391\n",
      "252/500 train loss: 0.8987\n",
      "253/500 train loss: 0.9521\n",
      "254/500 train loss: 0.9507\n",
      "255/500 train loss: 0.9689\n",
      "256/500 train loss: 0.9568\n",
      "257/500 train loss: 0.9498\n",
      "258/500 train loss: 0.9465\n",
      "259/500 train loss: 0.9233\n",
      "260/500 train loss: 0.9752\n",
      "261/500 train loss: 0.9147\n",
      "262/500 train loss: 0.9616\n",
      "263/500 train loss: 0.9216\n",
      "264/500 train loss: 0.9408\n",
      "265/500 train loss: 0.9076\n",
      "266/500 train loss: 0.9762\n",
      "267/500 train loss: 0.9413\n",
      "268/500 train loss: 0.9312\n",
      "269/500 train loss: 0.9658\n",
      "270/500 train loss: 0.9473\n",
      "271/500 train loss: 0.9373\n",
      "272/500 train loss: 0.9667\n",
      "273/500 train loss: 0.9976\n",
      "274/500 train loss: 0.9353\n",
      "275/500 train loss: 0.9640\n",
      "276/500 train loss: 0.9715\n",
      "277/500 train loss: 0.9349\n",
      "278/500 train loss: 0.9768\n",
      "279/500 train loss: 0.9391\n",
      "280/500 train loss: 0.9228\n",
      "281/500 train loss: 0.9840\n",
      "282/500 train loss: 0.9539\n",
      "283/500 train loss: 0.9104\n",
      "284/500 train loss: 0.9239\n",
      "285/500 train loss: 0.9328\n",
      "286/500 train loss: 0.8921\n",
      "287/500 train loss: 0.9328\n",
      "288/500 train loss: 0.9558\n",
      "289/500 train loss: 0.9981\n",
      "290/500 train loss: 0.9823\n",
      "291/500 train loss: 0.9236\n",
      "292/500 train loss: 0.9371\n",
      "293/500 train loss: 0.9347\n",
      "294/500 train loss: 0.9831\n",
      "295/500 train loss: 0.9488\n",
      "296/500 train loss: 0.9360\n",
      "297/500 train loss: 0.9501\n",
      "298/500 train loss: 0.9447\n",
      "299/500 train loss: 0.9499\n",
      "300/500 train loss: 0.9423\n",
      "[validation loss] 1.0220 (accuracy 0.4564)\n",
      "301/500 train loss: 0.9406\n",
      "302/500 train loss: 0.9925\n",
      "303/500 train loss: 0.9384\n",
      "304/500 train loss: 0.9466\n",
      "305/500 train loss: 0.9760\n",
      "306/500 train loss: 0.9258\n",
      "307/500 train loss: 0.9593\n",
      "308/500 train loss: 0.9365\n",
      "309/500 train loss: 0.9859\n",
      "310/500 train loss: 0.9240\n",
      "311/500 train loss: 0.9556\n",
      "312/500 train loss: 0.9452\n",
      "313/500 train loss: 0.9997\n",
      "314/500 train loss: 0.9627\n",
      "315/500 train loss: 0.9326\n",
      "316/500 train loss: 0.9524\n",
      "317/500 train loss: 0.8977\n",
      "318/500 train loss: 0.9694\n",
      "319/500 train loss: 0.9412\n",
      "320/500 train loss: 0.9205\n",
      "321/500 train loss: 0.9992\n",
      "322/500 train loss: 0.9516\n",
      "323/500 train loss: 0.9454\n",
      "324/500 train loss: 0.9170\n",
      "325/500 train loss: 0.9666\n",
      "326/500 train loss: 0.9630\n",
      "327/500 train loss: 0.9689\n",
      "328/500 train loss: 0.9171\n",
      "329/500 train loss: 0.9759\n",
      "330/500 train loss: 0.9309\n",
      "331/500 train loss: 0.9559\n",
      "332/500 train loss: 0.9765\n",
      "333/500 train loss: 0.9685\n",
      "334/500 train loss: 0.9493\n",
      "335/500 train loss: 0.9537\n",
      "336/500 train loss: 0.9634\n",
      "337/500 train loss: 0.9301\n",
      "338/500 train loss: 0.9390\n",
      "339/500 train loss: 0.9873\n",
      "340/500 train loss: 0.9222\n",
      "341/500 train loss: 0.9781\n",
      "342/500 train loss: 0.9767\n",
      "343/500 train loss: 0.9311\n",
      "344/500 train loss: 0.9321\n",
      "345/500 train loss: 0.8939\n",
      "346/500 train loss: 0.9605\n",
      "347/500 train loss: 0.9667\n",
      "348/500 train loss: 0.9803\n",
      "349/500 train loss: 0.9561\n",
      "350/500 train loss: 0.9349\n",
      "351/500 train loss: 0.9732\n",
      "352/500 train loss: 0.9635\n",
      "353/500 train loss: 0.9432\n",
      "354/500 train loss: 0.9627\n",
      "355/500 train loss: 0.9475\n",
      "356/500 train loss: 0.9609\n",
      "357/500 train loss: 0.9719\n",
      "358/500 train loss: 0.9550\n",
      "359/500 train loss: 0.9095\n",
      "360/500 train loss: 0.9597\n",
      "361/500 train loss: 0.9302\n",
      "362/500 train loss: 0.9369\n",
      "363/500 train loss: 0.9448\n",
      "364/500 train loss: 0.9371\n",
      "365/500 train loss: 0.9262\n",
      "366/500 train loss: 0.9217\n",
      "367/500 train loss: 0.9660\n",
      "368/500 train loss: 0.9442\n",
      "369/500 train loss: 0.9224\n",
      "370/500 train loss: 0.9932\n",
      "371/500 train loss: 0.9644\n",
      "372/500 train loss: 0.9211\n",
      "373/500 train loss: 0.9249\n",
      "374/500 train loss: 0.9202\n",
      "375/500 train loss: 0.9136\n",
      "376/500 train loss: 0.8992\n",
      "377/500 train loss: 0.9298\n",
      "378/500 train loss: 0.9197\n",
      "379/500 train loss: 0.9578\n",
      "380/500 train loss: 0.9343\n",
      "381/500 train loss: 0.9558\n",
      "382/500 train loss: 0.9716\n",
      "383/500 train loss: 0.9264\n",
      "384/500 train loss: 0.9275\n",
      "385/500 train loss: 0.9638\n",
      "386/500 train loss: 0.9511\n",
      "387/500 train loss: 0.9593\n",
      "388/500 train loss: 0.9658\n",
      "389/500 train loss: 0.9215\n",
      "390/500 train loss: 0.9089\n",
      "391/500 train loss: 0.9485\n",
      "392/500 train loss: 0.9096\n",
      "393/500 train loss: 0.9482\n",
      "394/500 train loss: 0.9751\n",
      "395/500 train loss: 0.9600\n",
      "396/500 train loss: 0.9453\n",
      "397/500 train loss: 0.9545\n",
      "398/500 train loss: 0.9504\n",
      "399/500 train loss: 0.9180\n",
      "400/500 train loss: 0.9362\n",
      "[validation loss] 1.0581 (accuracy 0.4849)\n",
      "401/500 train loss: 0.9302\n",
      "402/500 train loss: 0.9744\n",
      "403/500 train loss: 0.9878\n",
      "404/500 train loss: 0.9348\n",
      "405/500 train loss: 0.9958\n",
      "406/500 train loss: 0.8556\n",
      "407/500 train loss: 0.9041\n",
      "408/500 train loss: 0.9405\n",
      "409/500 train loss: 0.8979\n",
      "410/500 train loss: 0.8982\n",
      "411/500 train loss: 0.8976\n",
      "412/500 train loss: 0.9730\n",
      "413/500 train loss: 1.0112\n",
      "414/500 train loss: 0.9823\n",
      "415/500 train loss: 0.9236\n",
      "416/500 train loss: 0.9481\n",
      "417/500 train loss: 0.9697\n",
      "418/500 train loss: 0.9246\n",
      "419/500 train loss: 0.9559\n",
      "420/500 train loss: 0.9490\n",
      "421/500 train loss: 0.9335\n",
      "422/500 train loss: 0.9109\n",
      "423/500 train loss: 0.9557\n",
      "424/500 train loss: 0.9392\n",
      "425/500 train loss: 0.9549\n",
      "426/500 train loss: 0.9519\n",
      "427/500 train loss: 0.9478\n",
      "428/500 train loss: 0.9723\n",
      "429/500 train loss: 0.9778\n",
      "430/500 train loss: 0.9083\n",
      "431/500 train loss: 0.9257\n",
      "432/500 train loss: 0.9263\n",
      "433/500 train loss: 0.9389\n",
      "434/500 train loss: 0.9388\n",
      "435/500 train loss: 0.9898\n",
      "436/500 train loss: 0.9518\n",
      "437/500 train loss: 0.9204\n",
      "438/500 train loss: 0.9060\n",
      "439/500 train loss: 0.9793\n",
      "440/500 train loss: 0.9416\n",
      "441/500 train loss: 0.9592\n",
      "442/500 train loss: 0.9480\n",
      "443/500 train loss: 0.9003\n",
      "444/500 train loss: 0.9393\n",
      "445/500 train loss: 0.9625\n",
      "446/500 train loss: 0.8960\n",
      "447/500 train loss: 0.9507\n",
      "448/500 train loss: 0.9373\n",
      "449/500 train loss: 0.9489\n",
      "450/500 train loss: 0.9367\n",
      "451/500 train loss: 0.9595\n",
      "452/500 train loss: 0.9197\n",
      "453/500 train loss: 0.9410\n",
      "454/500 train loss: 0.9689\n",
      "455/500 train loss: 0.9542\n",
      "456/500 train loss: 0.9063\n",
      "457/500 train loss: 0.9317\n",
      "458/500 train loss: 1.0303\n",
      "459/500 train loss: 0.9217\n",
      "460/500 train loss: 0.9160\n",
      "461/500 train loss: 0.9489\n",
      "462/500 train loss: 0.9686\n",
      "463/500 train loss: 0.9466\n",
      "464/500 train loss: 0.9689\n",
      "465/500 train loss: 0.9490\n",
      "466/500 train loss: 0.9301\n",
      "467/500 train loss: 0.9487\n",
      "468/500 train loss: 0.9176\n",
      "469/500 train loss: 0.9188\n",
      "470/500 train loss: 0.9114\n",
      "471/500 train loss: 0.9241\n",
      "472/500 train loss: 0.9227\n",
      "473/500 train loss: 0.9490\n",
      "474/500 train loss: 0.8879\n",
      "475/500 train loss: 0.9506\n",
      "476/500 train loss: 0.9541\n",
      "477/500 train loss: 0.9015\n",
      "478/500 train loss: 0.9100\n",
      "479/500 train loss: 0.9758\n",
      "480/500 train loss: 0.9310\n",
      "481/500 train loss: 0.9043\n",
      "482/500 train loss: 0.9685\n",
      "483/500 train loss: 0.9758\n",
      "484/500 train loss: 0.9466\n",
      "485/500 train loss: 0.9592\n",
      "486/500 train loss: 0.9743\n",
      "487/500 train loss: 0.8891\n",
      "488/500 train loss: 0.9866\n",
      "489/500 train loss: 0.9131\n",
      "490/500 train loss: 0.9366\n",
      "491/500 train loss: 0.9404\n",
      "492/500 train loss: 0.9417\n",
      "493/500 train loss: 0.9128\n",
      "494/500 train loss: 0.8978\n",
      "495/500 train loss: 0.9175\n",
      "496/500 train loss: 0.9210\n",
      "497/500 train loss: 0.9615\n",
      "498/500 train loss: 0.9848\n",
      "499/500 train loss: 0.9360\n",
      "500/500 train loss: 0.9600\n",
      "[validation loss] 1.0238 (accuracy 0.4780)\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "sess = tf.Session()\n",
    "sess.run(nn.initialize_all_variables())\n",
    "saver = tf.train.Saver()\n",
    "x_val, y_val, val_seq_len = val_x, val_y, val_seq\n",
    "train_writer.add_graph(nn.input.graph)\n",
    "\n",
    "for i in range(FLAGS.train_steps):\n",
    "    # Perform training step\n",
    "    x_train, y_train, train_seq_len = next(get_batches(train_x, train_y, train_seq, FLAGS.batch_size))\n",
    "    train_loss, _, summary = sess.run([nn.loss, nn.train_step, nn.merged],\n",
    "                                      feed_dict={nn.input: x_train,\n",
    "                                                 nn.target: y_train,\n",
    "                                                 nn.seq_len: train_seq_len,\n",
    "                                                 nn.dropout_keep_prob: FLAGS.dropout_keep_prob})\n",
    "    train_writer.add_summary(summary, i)  # Write train summary for step i (TensorBoard visualization)\n",
    "    print('{0}/{1} train loss: {2:.4f}'.format(i + 1, FLAGS.train_steps, train_loss))\n",
    "\n",
    "    # Check validation performance\n",
    "    if (i + 1) % FLAGS.validate_every == 0:\n",
    "        val_loss, accuracy, summary = sess.run([nn.loss, nn.accuracy, nn.merged],\n",
    "                                               feed_dict={nn.input: x_val,\n",
    "                                                          nn.target: y_val,\n",
    "                                                          nn.seq_len: val_seq_len,\n",
    "                                                          nn.dropout_keep_prob: 1})\n",
    "        validation_writer.add_summary(summary, i)  # Write validation summary for step i (TensorBoard visualization)\n",
    "        print('[validation loss] {0:.4f} (accuracy {1:.4f})'.format(val_loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
